import psycopg2
import pandas as pd
import sys
import os
import json
import hashlib
from datetime import datetime, timedelta
import logging
from psycopg2 import sql
from typing import Optional, Dict, Any, List # Added type hints

# Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
LOG = logging.getLogger("s3_llm_integration")

# Relative path hack kept to maintain original import functionality
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../')))
from app.core.genai import llm_call
from app.ingestion.aws.postgres_operations import connection, dump_to_postgresql, fetch_existing_hash_keys
from app.ingestion.aws.pricing_helpers import (
    get_s3_storage_class_pricing,
    format_s3_pricing_for_llm
)
from sqlalchemy import create_engine
from urllib.parse import quote_plus
from dotenv import load_dotenv

load_dotenv()

# --- Utility Functions (Preserved/Removed for brevity) ---
# ... (Assuming _create_local_engine_from_env is defined elsewhere or not strictly needed here) ...


@connection
def fetch_s3_bucket_utilization_data(conn, schema_name, start_date, end_date, bucket_name=None):
    """
    Fetch S3 bucket metrics (all metrics) from gold metrics view and billing/pricing
    fields from the gold_aws_fact_focus view. Calculates AVG, MAX, and MAX Date for metrics.
    """
    
    # Use parameterized query components
    bucket_filter_sql = sql.SQL("AND bm.bucket_name = %s") if bucket_name else sql.SQL("")

    # NOTE: The query now uses three CTEs to properly calculate aggregates and max date
    QUERY = sql.SQL("""
        WITH metric_agg AS (
            SELECT
                bm.bucket_name,
                bm.account_id,
                db.region,
                bm.metric_name,
                bm.value AS metric_value,
                bm.event_date
            FROM {schema_name}.fact_s3_metrics bm
            LEFT JOIN {schema_name}.dim_s3_bucket db
                ON bm.bucket_name = db.bucket_name
            WHERE
                bm.event_date BETWEEN %s AND %s
                {bucket_filter}
        ),

        max_date_lookup AS (
            SELECT DISTINCT ON (bucket_name, metric_name)
                bucket_name,
                metric_name,
                event_date AS max_date
            FROM metric_agg
            ORDER BY bucket_name, metric_name, metric_value DESC, event_date DESC
        ),

        usage_summary AS (
            SELECT
                m.bucket_name,
                m.account_id,
                m.region,
                m.metric_name,
                -- Convert bytes to GB for BucketSizeBytes metric
                AVG(
                    CASE
                        WHEN m.metric_name = 'BucketSizeBytes'
                        THEN m.metric_value / 1073741824.0  -- Convert bytes to GB (1024^3)
                        ELSE m.metric_value
                    END
                ) AS avg_value,
                MAX(
                    CASE
                        WHEN m.metric_name = 'BucketSizeBytes'
                        THEN m.metric_value / 1073741824.0  -- Convert bytes to GB (1024^3)
                        ELSE m.metric_value
                    END
                ) AS max_value,
                MAX(md.max_date) AS max_date
            FROM metric_agg m
            LEFT JOIN max_date_lookup md
                ON md.bucket_name = m.bucket_name
                AND md.metric_name = m.metric_name
            GROUP BY m.bucket_name, m.account_id, m.region, m.metric_name
        ),
        
        metric_map AS (
            SELECT
                bucket_name,
                -- Combine AVG, MAX value, and MAX date into a single JSON object per bucket
                -- Cast to jsonb for concatenation operator to work
                (
                    json_object_agg(
                        metric_name || '_Avg', ROUND(avg_value::numeric, 6)
                    )::jsonb ||
                    json_object_agg(
                        metric_name || '_Max', ROUND(max_value::numeric, 6)
                    )::jsonb ||
                    json_object_agg(
                        metric_name || '_MaxDate', TO_CHAR(max_date, 'YYYY-MM-DD')
                    )::jsonb
                )::json AS metrics_json
            FROM usage_summary
            GROUP BY 1
        )
        
        SELECT
            us.bucket_name,
            us.account_id,
            us.region,
            MAX(m.metrics_json::text)::json AS metrics_json,  -- Cast to text for MAX(), then back to json
            -- Pull cost fields from the focus table (assuming one cost record per bucket/period)
            MAX(ff.pricing_category) AS pricing_category,
            MAX(ff.pricing_unit) AS pricing_unit,
            MAX(ff.contracted_unit_price) AS contracted_unit_price,
            SUM(ff.billed_cost) AS billed_cost,
            SUM(ff.consumed_quantity) AS consumed_quantity,
            MAX(ff.consumed_unit) AS consumed_unit
        FROM usage_summary us
        LEFT JOIN metric_map m ON m.bucket_name = us.bucket_name
        LEFT JOIN {schema_name}.gold_aws_fact_focus ff
            -- Join cost on resource_id = bucket_name
            ON ff.resource_id = us.bucket_name
               AND ff.charge_period_start::date <= %s
               AND ff.charge_period_end::date >= %s
        GROUP BY 1, 2, 3 -- Group by bucket_name, account_id, region only
    """).format(
        schema_name=sql.Identifier(schema_name),
        bucket_filter=bucket_filter_sql
    )

    # Build params in the correct order to match the SQL placeholders
    params = [start_date, end_date]
    if bucket_name:
        params.append(bucket_name)  # Bucket filter comes after BETWEEN clause
    params.extend([end_date, start_date])  # For charge_period_start/end filters

    try:
        cursor = conn.cursor()
        cursor.execute(QUERY, params) 
        
        columns = [desc[0] for desc in cursor.description]
        data = cursor.fetchall()
        cursor.close()

        if not data:
            return pd.DataFrame()

        df = pd.DataFrame(data, columns=columns)
        
        # Expand the metrics_json into separate columns (flatten)
        if not df.empty and "metrics_json" in df.columns:
            metrics_expanded = pd.json_normalize(df["metrics_json"].fillna({})).add_prefix("metric_")
            metrics_expanded.index = df.index
            df = pd.concat([df.drop(columns=["metrics_json"]), metrics_expanded], axis=1)

        return df

    except psycopg2.Error as e:
        raise RuntimeError(f"PostgreSQL query failed: {e}") from e
    except Exception as e:
        raise RuntimeError(f"An unexpected error occurred during DB fetch: {e}") from e


def _format_s3_metrics_for_llm(bucket_data: dict) -> dict:
    """
    Format S3 metrics for LLM with proper units.
    Similar to Azure's metric formatting approach.
    """
    # Metrics that should be renamed to show GB units
    METRIC_DISPLAY_NAMES = {
        "BucketSizeBytes": "Bucket Size (GB)",
        "NumberOfObjects": "Number of Objects",
        "AllRequests": "All Requests",
        "GetRequests": "GET Requests",
        "PutRequests": "PUT Requests",
        "4xxErrors": "4xx Errors",
        "5xxErrors": "5xx Errors"
    }

    formatted_metrics = {}

    # Find all metric keys in the data
    for key in bucket_data.keys():
        if key.startswith('metric_') and key.endswith('_Avg'):
            # Extract metric name (e.g., "BucketSizeBytes" from "metric_BucketSizeBytes_Avg")
            metric_name = key.replace('metric_', '').replace('_Avg', '')

            # Get display name with units
            display_name = METRIC_DISPLAY_NAMES.get(metric_name, metric_name)

            # Build metric entry
            entry = {
                "Avg": bucket_data.get(f'metric_{metric_name}_Avg'),
                "Max": bucket_data.get(f'metric_{metric_name}_Max'),
                "MaxDate": bucket_data.get(f'metric_{metric_name}_MaxDate')
            }

            # Only include if at least one value is present
            if any(v is not None for v in entry.values()):
                formatted_metrics[display_name] = entry

    return formatted_metrics


def _extrapolate_costs(billed_cost: float, duration_days: int) -> Dict[str, float]:
    """Helper to calculate monthly/annual forecasts."""
    if duration_days == 0:
        return {"monthly": 0.0, "annually": 0.0}

    avg_daily_cost = billed_cost / duration_days
    print(f"Avg daily cost calculated: {avg_daily_cost}")
    # Use 30.4375 for average days in a month (365.25 / 12)
    monthly = avg_daily_cost * 30.4375
    annually = avg_daily_cost * 365
    print(f"Extrapolated monthly: {monthly}, annually: {annually}")
    return {"monthly": monthly, "annually": annually}


def generate_s3_prompt(bucket_data: dict, monthly_forecast: float, annual_forecast: float) -> str:
    """
    Generate LLM prompt for S3 bucket optimization recommendations.

    Args:
        bucket_data: Dictionary containing bucket metrics and cost data
        monthly_forecast: Extrapolated monthly cost forecast
        annual_forecast: Extrapolated annual cost forecast

    Returns:
        Formatted prompt string for the LLM
    """
    bucket_name = bucket_data.get('bucket_name', 'Unknown')
    region = bucket_data.get('region', 'Unknown')
    account_id = bucket_data.get('account_id', 'Unknown')
    billed_cost = bucket_data.get('billed_cost', 0)

    start_date = bucket_data.get('start_date', 'N/A')
    end_date = bucket_data.get('end_date', 'N/A')
    duration_days = bucket_data.get('duration_days', 0)

    # Format metrics with proper units
    formatted_metrics = _format_s3_metrics_for_llm(bucket_data)

    # Fetch S3 storage class pricing from database
    schema_name = bucket_data.get('schema_name', '')

    pricing_context = ""
    has_pricing = False
    if schema_name:
        try:
            s3_pricing = get_s3_storage_class_pricing(schema_name, region)
            # Assume current storage class is S3 Standard (most common default)
            current_class = "STANDARD"

            # Debug: Print fetched pricing
            print(f"\n{'='*60}")
            print(f"PRICING DEBUG - AWS S3: {bucket_name} in {region}")
            print(f"{'='*60}")
            print(f"CURRENT STORAGE CLASS: {current_class} (assumed)")

            if s3_pricing and len(s3_pricing) > 0:
                has_pricing = True
                print(f"\nS3 STORAGE CLASS PRICING (Top 5):")
                for idx, (storage_class, info) in enumerate(list(s3_pricing.items())[:5], 1):
                    marker = "‚Üê CURRENT" if storage_class == current_class else ""
                    print(f"  {idx}. {storage_class}: {info['price_per_unit']:.6f} per {info['unit']} {marker}")
            else:
                print(f"\nS3 STORAGE CLASS PRICING: Not found in database")
                print(f"Using fallback pricing estimates")

                # Fallback pricing for common S3 storage classes
                has_pricing = True
                s3_pricing = {
                    'STANDARD': {'storage_class': 'STANDARD', 'price_per_unit': 0.023, 'unit': 'GB', 'description': 'S3 Standard Storage'},
                    'STANDARD_IA': {'storage_class': 'STANDARD_IA', 'price_per_unit': 0.0125, 'unit': 'GB', 'description': 'S3 Standard-IA Storage'},
                    'INTELLIGENT_TIERING': {'storage_class': 'INTELLIGENT_TIERING', 'price_per_unit': 0.023, 'unit': 'GB', 'description': 'S3 Intelligent-Tiering Storage'},
                    'ONEZONE_IA': {'storage_class': 'ONEZONE_IA', 'price_per_unit': 0.01, 'unit': 'GB', 'description': 'S3 One Zone-IA Storage'},
                    'GLACIER': {'storage_class': 'GLACIER', 'price_per_unit': 0.004, 'unit': 'GB', 'description': 'S3 Glacier Storage'},
                    'GLACIER_DEEP_ARCHIVE': {'storage_class': 'GLACIER_DEEP_ARCHIVE', 'price_per_unit': 0.00099, 'unit': 'GB', 'description': 'S3 Glacier Deep Archive Storage'}
                }

                print(f"  Standard: 0.023 per GB (estimated)")
                print(f"  Standard-IA: 0.0125 per GB (estimated)")
                print(f"  Glacier: 0.004 per GB (estimated)")
                print(f"  Glacier Deep Archive: 0.00099 per GB (estimated)")
                print(f"  (Fallback estimates - actual pricing unavailable)")

            print(f"{'='*60}\n")

            pricing_context = "\n\n" + format_s3_pricing_for_llm(s3_pricing, current_class) + "\n"
        except Exception as e:
            LOG.warning(f"Could not fetch S3 pricing: {e}")
            import traceback
            traceback.print_exc()

            # Fallback pricing on error
            has_pricing = True
            pricing_context = f"\n\nPRICING DATA (ESTIMATED - Database unavailable):\nStandard: ~0.023 USD/GB, Standard-IA: ~0.0125 USD/GB, Glacier: ~0.004 USD/GB\n"

    # Check if we have metrics available
    has_metrics = bool(formatted_metrics)

    # Build metrics list for base_of_recommendations (with units and quotes)
    metrics_list = []
    for metric_name, values in formatted_metrics.items():
        if values.get('Avg') is not None:
            # Extract unit from metric name if it has one
            if 'Size' in metric_name or 'Bytes' in metric_name or 'Storage' in metric_name:
                unit = 'GB'
            elif 'Requests' in metric_name or 'Count' in metric_name:
                unit = 'count'
            elif 'Latency' in metric_name:
                unit = 'ms'
            else:
                unit = ''

            if unit:
                metrics_list.append(f'"{metric_name}: Avg={values["Avg"]:.2f}{unit}, Max={values.get("Max", 0):.2f}{unit}"')
            else:
                metrics_list.append(f'"{metric_name}: Avg={values["Avg"]:.2f}, Max={values.get("Max", 0):.2f}"')

    metrics_list_str = '[' + ', '.join(metrics_list) + ']' if metrics_list else '[]'

    # Build explicit metric summary (with units)
    metrics_summary = []
    for metric_name, values in formatted_metrics.items():
        if values.get('Avg') is not None:
            if 'Size' in metric_name or 'Bytes' in metric_name or 'Storage' in metric_name:
                unit = 'GB'
            elif 'Requests' in metric_name or 'Count' in metric_name:
                unit = 'count'
            elif 'Latency' in metric_name:
                unit = 'ms'
            else:
                unit = ''

            if unit:
                metrics_summary.append(f"- {metric_name}: Avg={values['Avg']:.2f}{unit}, Max={values.get('Max', 0):.2f}{unit}, MaxDate={values.get('MaxDate', 'N/A')}")
            else:
                metrics_summary.append(f"- {metric_name}: Avg={values['Avg']:.2f}, Max={values.get('Max', 0):.2f}, MaxDate={values.get('MaxDate', 'N/A')}")

    metrics_text = "\n".join(metrics_summary) if metrics_summary else "No metrics available"

    # Guard clause for missing data
    if not has_pricing or not has_metrics or bucket_name == "Unknown" or bucket_name == "None":
        return f"""AWS S3 {bucket_name} | {region} | {duration_days}d | ${billed_cost:.2f}
ISSUE: {'No pricing' if not has_pricing else 'No metrics' if not has_metrics else 'Unknown bucket'}
OUTPUT: {{"recommendations": {{"effective_recommendation": {{"text": "Cannot recommend", "explanation": "Insufficient data", "saving_pct": 0}}, "additional_recommendation": [], "base_of_recommendations": {metrics_list_str}}}, "cost_forecasting": {{"monthly": {monthly_forecast:.2f}, "annually": {annual_forecast:.2f}}}, "anomalies": [], "contract_deal": {{"assessment": "unknown", "for_sku": "S3 Standard", "reason": "Insufficient data", "monthly_saving_pct": 0, "annual_saving_pct": 0}}}}"""

    prompt = f"""AWS S3 {bucket_name} | {region} | {duration_days}d | ${monthly_forecast:.2f}/mo

METRICS:
{metrics_text}

PRICING:
{pricing_context}

RULES:
1. Cite metrics with units
2. Alt class cost = class_price_per_GB √ó capacity_GB
3. savings_pct = (forecast - alt_cost) / forecast √ó 100
4. CRITICAL: Each recommendation must be DIFFERENT ACTION CATEGORY. Do NOT give same action 3 times (e.g., NOT three storage class changes). Consider: storage class changes, lifecycle policies, versioning configs, replication settings
5. Anomalies: MaxDate + reason
6. contract_deal: reserved capacity vs on-demand for Standard class only

OUTPUT (JSON):
{{
  "recommendations": {{
    "effective_recommendation": {{"text": "Action", "explanation": "Metrics + cost calc", "saving_pct": <num>}},
    "additional_recommendation": [
      {{"text": "Unique type", "explanation": "Metrics + cost calc", "saving_pct": <num>}},
      {{"text": "Another unique type", "explanation": "Metrics + cost calc", "saving_pct": <num>}}
    ],
    "base_of_recommendations": {metrics_list_str}
  }},
  "cost_forecasting": {{"monthly": {monthly_forecast:.2f}, "annually": {annual_forecast:.2f}}},
  "anomalies": [{{"metric_name": "Name", "timestamp": "MaxDate", "value": <num>, "reason_short": "Why unusual"}}],
  "contract_deal": {{"assessment": "good|bad|unknown", "for_sku": "S3 Standard", "reason": "Reserved capacity vs on-demand for Standard", "monthly_saving_pct": <num>, "annual_saving_pct": <num>}}
}}"""
    return prompt


def get_s3_recommendation_single(bucket_data: dict) -> dict:
    """
    Get LLM recommendation for a single S3 bucket.

    Args:
        bucket_data: Dictionary containing bucket metrics

    Returns:
        Dictionary containing recommendations or None if error
    """
    try:
        # Calculate cost forecasts
        billed_cost = bucket_data.get('billed_cost', 0.0)
        duration_days = int(bucket_data.get('duration_days', 30) or 30)
        forecast = _extrapolate_costs(billed_cost, duration_days)

        # Generate prompt with forecasts
        prompt = generate_s3_prompt(bucket_data, forecast['monthly'], forecast['annually'])
        llm_response = llm_call(prompt)

        if not llm_response:
            LOG.warning(f"Empty LLM response for bucket {bucket_data.get('bucket_name')}")
            return None

        # Try to parse as JSON directly
        try:
            # Remove markdown code blocks if present
            if '```json' in llm_response:
                llm_response = llm_response.split('```json')[1].split('```')[0].strip()
            elif '```' in llm_response:
                llm_response = llm_response.split('```')[1].split('```')[0].strip()

            recommendation = json.loads(llm_response)
            # Add resource_id and forecasts to the recommendation
            recommendation['resource_id'] = bucket_data.get('bucket_name', 'Unknown')
            recommendation['_forecast_monthly'] = forecast['monthly']
            recommendation['_forecast_annual'] = forecast['annually']
            return recommendation
        except json.JSONDecodeError:
            LOG.warning(f"Failed to parse JSON for bucket {bucket_data.get('bucket_name')}")
            return None

    except Exception as e:
        LOG.error(f"Error getting S3 recommendation: {e}")
        return None


# --- run_llm_analysis_s3 (No change needed here, it uses the fetch function) ---

def run_llm_analysis_s3(schema_name, start_date=None, end_date=None, bucket_name=None):
  
    start_str = start_date or (datetime.utcnow().date() - timedelta(days=7)).strftime("%Y-%m-%d")
    end_str = end_date or datetime.utcnow().date().strftime("%Y-%m-%d")

    LOG.info(f"üöÄ Starting S3 LLM analysis from {start_str} to {end_str}...")

    df = None
    try:
        # The fetch function is decorated with @connection, but needs to be called carefully
        # Note: If @connection isn't handling the conn argument internally, you need to manually pass it or update the decorator.
        # Assuming the @connection decorator handles the connection context:
        df = fetch_s3_bucket_utilization_data(schema_name, start_str, end_str, bucket_name)
    except RuntimeError as e:
        LOG.error(f"‚ùå Failed to fetch S3 utilization data: {e}")
        return
    except Exception as e:
        LOG.error(f"‚ùå An unhandled error occurred during data fetching: {e}")
        return

    if df is None or df.empty:
        LOG.warning("‚ö†Ô∏è No S3 bucket data found for the requested date range / bucket.")
        return

    LOG.info(f"üìà Retrieved data for {len(df)} bucket(s)")

    # Annotate with date info for LLM context
    df["start_date"] = start_str
    df["end_date"] = end_str
    df["duration_days"] = (pd.to_datetime(end_str) - pd.to_datetime(start_str)).days

    # Convert to list-of-dicts for LLM helper
    buckets = df.to_dict(orient="records")

    LOG.info("ü§ñ Calling LLM for S3 recommendations...")
    recommendations = []

    for bucket_data in buckets:
        # Add schema_name and region for pricing lookups
        bucket_data['schema_name'] = schema_name
        bucket_data['region'] = bucket_data.get('region', 'us-east-1')
        rec = get_s3_recommendation_single(bucket_data)
        if rec:
            recommendations.append(rec)

    if recommendations:
        LOG.info(f"‚úÖ S3 analysis complete! Generated {len(recommendations)} recommendation(s).")
        return recommendations
    else:
        LOG.warning("‚ö†Ô∏è No recommendations generated by LLM.")
        return []
